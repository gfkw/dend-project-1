# Project 1: Data Modeling with Postgres
---

## Project Summary

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The purpose of this database is to provide Sparkify the ability to easily query data in a timely manner so they can come up with a big picture on user behavior. Once Sparkify gets a better understanding on how users are using their app, they will be able to trade-off a better experience in getting new music suggestions and songs selection.

## Data Modeling

In order to afford Sparkify's analytics requirements, the data engineering team understands that the best approach is the star schema modeling. The star schema will allow a single fact table to keep track of user's interactions with the app. Also, this modeling will make the fact table to be surrounded by some dimensions that categorizes entities like users and songs. It's a straightforward data model that allows for redundancy to promote best performance.

The following illustrates the data model used in this project:

![Data Model](/images/star_schema_sparkify.png)

## Data Sources

Data resides in two directories that contain files in JSON format:

1. **data/song_data** : Contains metadata about a song and the artist of that song;
2. **data/log_data** : Consists of log files generated by the streaming app based on the songs in the dataset above;

## Data Quality Checks

Analytics are best performed on data with quality standards, so the following data quality actions were taken in this project:

1. NaN's and blank spaces were replaced to `null`;
2. Duplicate removal on dimension tables. 

## Scripts Usage

***Important note:*** The files were ran on local machine with Postgres Server running on port = 8080

1. **etl.py**: Responsible for the orchestration of the entire data flow pipeline that will execute the extraction from JSON source files, transform data with DQ checks and load into Postgres tables;
2. **create_table.py** : Database and tables creation. It is necessary to be run before etl.py so the tables are created and cleaned;
3. **sql_queries.py** : Contains the creation table DDL and inserts DML scripts that will be called by create_table.py. It will drop any existing table before creating it again.
